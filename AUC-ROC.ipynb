{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix,\\\n",
    "    precision_score, recall_score, accuracy_score, f1_score, log_loss,\\\n",
    "    roc_curve, roc_auc_score, classification_report, plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "- Calculate and interpret probability estimates\n",
    "- Adjust the threshold of a logistic regression model\n",
    "- Visualize, calculate and interpret the AUC-ROC metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Now that we've learned how to evaluate a classification model's predictions, let's dig deeper to see how else we might evaluate our models and how we can use that information to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: Identifying Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use [this Kaggle dataset](https://www.kaggle.com/ronitf/heart-disease-uci) about predicting heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_data = pd.read_csv('heart.csv')\n",
    "hd_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into feature and target DataFrames\n",
    "hd_X = hd_data.drop('target', axis = 1)\n",
    "hd_y = hd_data['target']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(hd_X, hd_y, test_size=.25,\n",
    "                                                   random_state=1)\n",
    "# Scale the data for modeling\n",
    "hd_scaler = StandardScaler()\n",
    "hd_scaler.fit(X_train)\n",
    "X_train_sc = hd_scaler.transform(X_train)\n",
    "X_test_sc = hd_scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regresssion model with the train data\n",
    "hd_model = LogisticRegression(random_state=42)\n",
    "hd_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Labels\n",
    "\n",
    "Let's look at some predictions from our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hd_model.predict(X_test_sc)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the `.predict()` method, `sklearn` gives us the predicted values for each transaction in our test set: 0 if predicting \"not fraudulent\", 1 if predicting \"fraudulent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Probability Estimates\n",
    "\n",
    "If you remember how the logistic regression model works, though, it doesn't actually generate predicted values of 0 or 1. It creates an S-shaped curve to approximate the data, estimating the _probability_ that they belong to the target class. This probability takes a value _between_ 0 and 1.\n",
    "\n",
    "![](https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_38a8acae.png)\n",
    "\n",
    "Source: [GraphPad](https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_simple_logistic_and_linear_difference.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get these estimated probabilities using the `.predict_proba()` method. Each element gives two probabilities: the estimate probability of being in the 0 class (not heart disease) and the 1 class (heart disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = hd_model.predict_proba(X_test_sc)\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholds\n",
    "\n",
    "How did we get those 0 and 1 label predictions, when the model only calculates probabilities between 0 and 1? \n",
    "\n",
    "Now the default behavior is simply to take the larger of these values as the \"real\" prediction. Since $0.996 > 0.004$, we'll understand the model to be predicting this point to belong to class \"0\" (or the negative class). An equivalent way of understanding the default behavior is that we:\n",
    "\n",
    "- round the predicted numbers up to 1 if they are at least as large as 0.5; and\n",
    "- round them down to 0 if they are less than 0.5.\n",
    "\n",
    "Since the probabilities must sum to 1, there will never be any problem with this algorithm. We refer to this value of 0.5 as the **threshold**.\n",
    "\n",
    "But we don't have to do things this way. Suppose we're building a model that predicts the presence of cancer from X-ray scans. And suppose we get a pair of probabilities for some particular scan that look like this:\n",
    "\n",
    "- pred_neg: 0.52, pred_pos: 0.48\n",
    "\n",
    "Because false negatives (cancers not flagged) are *much* more costly than false positives (non-cancers flagged as cancers), we may well want to **adjust our threshold**. We might want to have our model predict \"positive\" if the corresponding probability is, say, as low as 0.4, or maybe even as low as 0.1. (Speaking for myself, if there was even a 10% chance that I had cancer, I think I'd probably want to know about it.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True & False Positive Rates\n",
    "\n",
    "Adjusting the threshold can increase or decrease performance on different evaluation metrics. When doing this, data scientist often look at changes in two metrics: **True Positive Rate (TPR)** and **False Positive Rate (FPR)**. Let's define and calculate these. \n",
    "\n",
    "To do this, we'll first need to get the values from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, hd_model.predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate\n",
    "\n",
    "True Positive Rate (TPR) is the same as recall, measuring how many of the positive cases we correctly classified as positive.\n",
    "\n",
    "**True Positive Rate (TPR)** = **Recall** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "How many of the patients with heart disease did my model identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "print(tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate\n",
    "\n",
    "False Positive Rate (FPR) measures how many of the negative casses we incorrectly classified as positive.\n",
    "\n",
    "**False Positive Rate (TPR)** = $\\frac{FP}{FP + TN}$\n",
    "\n",
    "How many of the patients without heart disease did my model flag as having heart disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fp / (fp + tn)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the threshold\n",
    "\n",
    "The true- and false-positive rates will change if we make adjustments to the threshold. In fact, in the present case that was the whole point of making the adjustment: We want to minimize our false negatives.\n",
    "\n",
    "So this is how the plot of these rates takes shape.\n",
    "\n",
    "Let's build a function that will take in our data, together with a threshold setting, and return the corresponding true- and false-positive rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Receiver Operating Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve plots the true-positive rate vs. the false-positive rate. Let's define these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rates(y_test, y_probs, model, thresh):\n",
    "    y_hat = []\n",
    "    for val in y_probs:                                       # Each element in y_hat_probs is an array.\n",
    "        if val[0] < thresh:                                 # We'll set our own threshold for classifying\n",
    "            y_hat.append(1)                                 # a test point as positive! The lower my threshold,\n",
    "        else:                                               # the fewer predicted positives I'll have. For the\n",
    "            y_hat.append(0)                                 # cancer example, I'd want to set a *high* threshold.\n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    return tpr, fpr, f'tpr:{round(tpr, 3)}, fpr:{round(fpr, 3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True- and false-positive rates for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.linspace(0, 1, 11):\n",
    "    print(f'Rates at threshold = {round(x, 2)}: ' + classify_rates(y_test, y_prob, hd_model, x)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my threshold goes up, I'll have more positive predictions, which means I'll have both more true positives and more false positives.\n",
    "\n",
    "Note:\n",
    "\n",
    "- I can artificially increase my true-positive rate to 1 by setting my threshold to 1, but at that point my false-positive rate is also 1! I'll have no true negatives and no false negatives. This will arise naturally if my training data has **very few (actual) negatives**. \n",
    "- I can artificially reduce my false-positive rate to 0 by setting my threshold to 0, but at  that point my true-positive rate is also 0! I'll have no true positives and no false positives. This will arise naturally if my training data has **very few (actual)  positives**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the Curve (AUC)\n",
    "\n",
    "The ROC curve will be a plot of tpr (on the y-axis) vs. fpr (on the x-axis). There will always be a point at (0, 0) and another at (1, 1). The question is what happens in the middle. Since we want our y-values to be as high as possible for any particular x-value, a natural metric is to calculate the **area under the curve**. The larger the area, the better the classifier. The maximum possible area is the area of the whole box between 0 and 1 on both axes, so that's a **maximum area of 1**.\n",
    "\n",
    "What's the minimum? Well that depends on the ratios of (actual) positive and negatives in my data, in much the way that a baseline accuracy score does.\n",
    "\n",
    "Remember: If my test data comprises 90% positives and only 10% negatives, then a simple classifier that always predicts \"positive\" will be 90% accurate! And so that would be the baseline level for a classifier on that data.\n",
    "\n",
    "If we have equal numbers of positives and negatives, then we can set an **absolute minimum area of 0.5**. That's the \"curve\" we'd get by plotting a straight diagonal line from (0, 0) to (1, 1).\n",
    "\n",
    "Why? The area under the curve really represents the test's ability to **discriminate** positives from negatives. Suppose I randomly took several pairs of points, one positive and one negative, and checked my test's predictions. The area under the curve represents a threshold-independent measure of how often my test would get the two predictions correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Curve\n",
    "\n",
    "Let's plot our own ROC curve. We'll create an array of different thresholds and use our `classify_rates()` function to get the true- and false-positive rates for each threshold.\n",
    "\n",
    "One way of choosing a threshold **independently of business concerns** is to select the point on the curve that is furthest from (1, 0), the \"worse-case\" point where our true-positive rate is 0 and our false-positive rate is 1. So let's find that point as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "fprs = []\n",
    "diffs = []\n",
    "for x in np.linspace(0, 1, 101):\n",
    "    fprs.append(classify_rates(y_test, y_prob, hd_model, x)[1])\n",
    "    tprs.append(classify_rates(y_test, y_prob, hd_model, x)[0])\n",
    "    diffs.append(np.sqrt(tprs[-1]**2 + (1-fprs[-1])**2))\n",
    "    \n",
    "max_dist = diffs.index(np.max(diffs))\n",
    "print(f\"\"\"With a threshold of {(max_dist - 1) / 100}: \\n\"\"\"\n",
    "      f\"\"\"\\tYou\\'ll have a True Positive Rate of {round(tprs[max_dist], 3)} \\n\"\"\"\n",
    "      f\"\"\"\\tand a False Positive Rate of {round(fprs[max_dist], 3)}\"\"\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fprs[:max_dist], tprs[:max_dist], 'r.')\n",
    "ax.plot(fprs[max_dist], tprs[max_dist], 'ko', ms=10)\n",
    "ax.plot(fprs[max_dist + 1:], tprs[max_dist + 1:], 'r.')\n",
    "ax.plot(fprs, fprs, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn's `roc_auc_score()` function will compute the area under the curve for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the probabilitiy predictions for the \"1\" class (heart disease)\n",
    "y_hat_hd = y_prob[:, 1]\n",
    "\n",
    "roc_auc_score(y_test, y_hat_hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our curve with scikit-learn's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the FPR and TPR data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_hat_hd)\n",
    "\n",
    "# Plot the FPR and TPR data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_roc_curve()`\n",
    "\n",
    "You can also use the `plot_roc_curve()` function with just your fitted model and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(hd_model, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Visualizing Threshold Changes\n",
    "\n",
    "This [ROC Applet](https://web.archive.org/web/20210210014824/http://www.navan.name/roc/) helps  visualize how a change in the threshold corresponds to moving along the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: Breast Cancer Prediction\n",
    "\n",
    "Let's evaulate a model using scikit-learn's breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "preds, target = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(preds, target,\n",
    "                                                   random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "bc_scaler = StandardScaler()\n",
    "bc_scaler.fit(X_train)\n",
    "X_train_sc = bc_scaler.transform(X_train)\n",
    "X_test_sc = bc_scaler.transform(X_test)\n",
    "\n",
    "# Run the model\n",
    "bc_model = LogisticRegression(solver='lbfgs', max_iter=10000,\n",
    "                           random_state=42)\n",
    "bc_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "For this example, draw the ROC curve and calculate the AUC-ROC metric. Based on the results, do you think your model would be useful for identifying patients with breast cancer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "\n",
    "One of the most effective strategies to handle these cases is **oversampling the minority class**. That is, I give myself more data points than I really have. I could achieve this either by [bootstrapping](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) or by generating some data that is fake but close to actual data. The latter is the idea behind [SMOTE](https://imbalanced-learn.org/stable/over_sampling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Another more \"natural\" way of measuring the quality of a classifier is just to look at the loss function, which will often be the **log loss**. In multiclass problems, we use **cross-entropy loss**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(hd_X, hd_y, test_size=.25,\n",
    "                                                   random_state=1)\n",
    "\n",
    "log_loss(y_test, y_hat_hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While such loss values are difficult to interpret on their own, they are useful for comparing models. Models with lower loss values generate probability estimates that are closer to the true values, and thus are likely to perform better on many metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Log Loss by Hand\n",
    "\n",
    "Log loss is generally calculated as an average per data point, and is computed as follows:\n",
    "\n",
    "$L(y, \\hat{y}) = -\\frac{1}{N}\\sum^N_{i=1}[y_i\\ln(\\hat{y_i}) + (1-y_i)\\ln(1-\\hat{y_i})]$,\n",
    "\n",
    "where $y$ is the vector of true values and $\\hat{y}$ is the vector of probabilities that the point in question has a correct label of 1.\n",
    "\n",
    "- Suppose, for a given data point, that the correct prediction of the label is **0**. In that case, the contribution from that point to the sum in the loss function defined above will be $-\\ln(1-\\hat{y_i})$. So, the closer the prediction for that point is to 0, the closer the contribution to the sum will be to $-\\ln(1)=0$. But as the prediction gets closer to 1, the closer the contribution will be to $-\\ln(0)=\\infty$.\n",
    "\n",
    "- Suppose, on the other hand, that the correct prediction is **1**. In that case, the contribution from that point to the sum in the loss function defined above will be $-\\ln(\\hat{y_i})$. So, the closer the prediction for that point is to 1, the closer the contribution to the sum will be to $-\\ln(1)=0$. But as the prediction gets closer to 0, the closer the contribution will be to $-\\ln(0)=\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = list(zip(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = [-(yi * np.log(yi_hat[1]) + (1 - yi) * np.log(yi_hat[0])) for (yi, yi_hat) in compare]\n",
    "calc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(calc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
